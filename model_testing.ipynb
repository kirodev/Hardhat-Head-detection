{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnOJic45Q5PY"
      },
      "source": [
        "!pip install tensorflow=='2.6.0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ANlAO2YMc1t"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import six\n",
        "import time\n",
        "\n",
        "import imageio\n",
        "import glob\n",
        "import scipy\n",
        "import numpy as np\n",
        "#import cv2\n",
        "import csv\n",
        "import scipy.misc\n",
        "import pandas as pd\n",
        "import lxml.etree as ET\n",
        "\n",
        "\n",
        "import pathlib\n",
        "import hashlib\n",
        "\n",
        "from PIL import Image\n",
        "from six import BytesIO\n",
        "from IPython.display import display\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DklX1RSQxPG"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/wobot_final/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyYs6WLRMoKj"
      },
      "source": [
        "if 'models' in pathlib.Path.cwd().parts:\n",
        "  while 'models' in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVTPWn2xRQqi"
      },
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvbyvhERSU-f"
      },
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import tensorflow as tf\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiFbFpp7f80D"
      },
      "source": [
        "utils_ops.tf = tf.compat.v1\n",
        "\n",
        "# Patch the location of gfile\n",
        "tf.gfile = tf.io.gfile"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE-sxNcMR4Zi"
      },
      "source": [
        "labelmap_path = '/content/drive/MyDrive/wobot_final/models/research/object_detection/data/tf_label_map.pbtxt'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIdbt57GR6to"
      },
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-59dNq3SBk1"
      },
      "source": [
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.saved_model.load('/content/drive/MyDrive/wobot_final/models/research/object_detection/inference_graph/saved_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkQke0R3SChb"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: a file path (this can be local or on colossus)\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  if image.getdata().mode == \"RGBA\":\n",
        "    image = image.convert('RGB')\n",
        "  np_array = np.array(image.getdata())\n",
        "  reshaped = np_array.reshape((im_height, im_width, 3))\n",
        "  return reshaped.astype(np.uint8)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR4O1UYZD4EX"
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy() \n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "   \n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])      \n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "  return output_dict"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HNWLtt6SQK1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRJMvWIfl_Ug"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/wobot_final/New Folder/')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mhpFzFuQfnB"
      },
      "source": [
        "def csv_to_xml(image_file):\n",
        "  df1 = pd.DataFrame()\n",
        "  annot_file = image_file[53:-3]+'xml' \n",
        "  try:\n",
        "   open(f'annotations/{annot_file}', 'x')\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  df1 = pd.read_csv(f'csvS/{image_file[53:-3]}csv')\n",
        "  data = ET.Element('annotations')\n",
        "  item1 = ET.SubElement(data, 'folder')\n",
        "  item2 = ET.SubElement(data, 'filename')\n",
        "  item1.text = 'images'\n",
        "  item2.text = f'{image_file[53:]}'\n",
        "  for i in df1.itertuples():\n",
        "    item = ET.SubElement(data, 'object')\n",
        "    class_name = ET.SubElement(item, 'name')\n",
        "    bounding_box = ET.SubElement(item, 'bndbox')\n",
        "    x_min = ET.SubElement(bounding_box, 'xmin')\n",
        "    y_min = ET.SubElement(bounding_box, 'ymin')\n",
        "    x_max = ET.SubElement(bounding_box, 'xmax')\n",
        "    y_max = ET.SubElement(bounding_box, 'ymax')\n",
        "    class_name.text = i[6]\n",
        "    x_min.text = str(i[2])\n",
        "    y_min.text = str(i[3])\n",
        "    x_max.text = str(i[4])\n",
        "    y_max.text = str(i[5])\n",
        "\n",
        "\n",
        "\n",
        "  mydata = ET.tostring(data, pretty_print = True, xml_declaration = True, encoding = 'utf-8')\n",
        "  x = open(f'annotations/{annot_file}', 'wb')\n",
        "  x.write(mydata)\n",
        "  "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNrV9IdlSYi4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vth8-l3jIlqJ"
      },
      "source": [
        "color = {'Red':(255,0,0), 'Blue':(0,0,255), 'Green':(0,255,0),'Yellow':(255,255,0), 'White':(255,255,255), 'Orange':(255,102,0)}"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlG1uSF9SdEL"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl6G5GhNSi18"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3biM2sjf_0tC"
      },
      "source": [
        "def color_detection(image):\n",
        "  image_array = np.asarray(image)\n",
        "  \n",
        "  means = {}\n",
        "  #scale_percent = 2000\n",
        "  #width = int((image.shape[1]) * (scale_percent/100))\n",
        "  #height = int((image.shape[0])*(scale_percent/100))\n",
        "  sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
        "  path = \"ESPCN_x3.pb\"\n",
        "  model_name = 'espcn'\n",
        "  model_Scale = 3\n",
        "\n",
        "  sr.readModel(path)\n",
        "  sr.setModel(model_name, model_Scale)\n",
        "  result_img = sr.upsample(image_array)\n",
        "  #print(result_img)\n",
        "  img1=result_img.reshape((result_img.shape[1]*result_img.shape[0],3)) #flatten images\n",
        "  \n",
        "  #dim = (width, height)\n",
        "  #resized_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
        "  #resized_image.reshape((resized_image.shape[1]*resized_image.shape[0],3))\n",
        "  kmeans = KMeans(n_clusters = 3)\n",
        "  kmeans.fit(img1)\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  centroid_n = centroids.mean(axis = 0)\n",
        "  for i in color.keys():\n",
        "    dist = distance.euclidean(centroid_n, color[i])\n",
        "    means[i] = dist\n",
        "  return color[min(means, key=means.get)]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6dpIIEzTFft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL59zADj5aRE"
      },
      "source": [
        "!pip install opencv-contrib-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Ia25CEC3vP"
      },
      "source": [
        "for image_path in glob.glob('/content/drive/MyDrive/wobot_final/New Folder/images/*.png'):\n",
        "  output = []\n",
        "  df = pd.DataFrame()\n",
        "  image_np = load_image_into_numpy_array(image_path)\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  dets = output_dict['detection_boxes']\n",
        "  classes = output_dict['detection_classes']\n",
        "  scores = output_dict['detection_scores']\n",
        "  im_height = image_np.shape[0]\n",
        "  im_width = image_np.shape[1]\n",
        "  labels = ['Helmet', 'Head', 'Person']\n",
        "\n",
        "  for i in range(len(dets[0])):\n",
        "    if (scores[i] > 0.2) and (len(classes) != 0):\n",
        "      y1, x1, y2, x2 = dets[i,:]\n",
        "      left, right, top, bottom = x1 * im_width, x2 * im_width, y1 * im_height, y2 * im_height\n",
        "      #img = Image.fromarray(image_np)\n",
        "      #cropped_image = img.crop((int(left),int(top),int(right),int(bottom)))\n",
        "      #bb_color = color_detection(cropped_image)\n",
        "      cv2.rectangle(image_np, (int(left),int(top)), (int(right),int(bottom)), (36,255,12), 2)\n",
        "      cv2.putText(image_np, str(labels[int(classes[i])-1])+\":\"+str(int(scores[i]*100)), (int(left), int(top-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "      output=[image_path[53:], int(left),int(top), int(right),int(bottom), str(labels[int(classes[i])-1]) ]\n",
        "      #print(output)\n",
        "      s = pd.Series(output, index=None)\n",
        "      df = df.append(s, ignore_index= True)\n",
        "  df.to_csv(f'csvS/{image_path[53:-3]}csv', index=None)\n",
        "  cv2.imwrite(f'result/annot_{image_path[53:-3]}png', image_np)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmuCOxbATQcT"
      },
      "source": [
        "for image_file in glob.glob('/content/drive/MyDrive/wobot_final/New Folder/images/*.png'):\n",
        "  csv_to_xml(image_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9BDb4INTU74"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/wobot_final/video/')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my6JbnLGTX1n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrBNGOycWyEL"
      },
      "source": [
        "def video_inference1(video_file):\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  out_path = os.path.join('/content/drive/MyDrive/wobot/video/', 'output1.webM')\n",
        "  size = (1280,720)\n",
        "  fps = 10\n",
        "  fourcc = cv2.VideoWriter_fourcc('V','P','8','0')\n",
        "  out = cv2.VideoWriter(out_path,fourcc, fps, size)\n",
        "  labels = ['Helmet', 'Head', 'Person']\n",
        "\n",
        "  while cap.isOpened():\n",
        "    ret, image_np = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    output_dict = run_inference_for_single_image(model, image_np)\n",
        "    dets = output_dict['detection_boxes']\n",
        "    classes = output_dict['detection_classes']\n",
        "    scores = output_dict['detection_scores']\n",
        "    im_height = image_np.shape[0]\n",
        "    im_width = image_np.shape[1]\n",
        "\n",
        "\n",
        "    for i in range(len(dets[0])):\n",
        "      if (scores[i] > 0.2) and (len(classes) != 0):\n",
        "        y1, x1, y2, x2 = dets[i,:]\n",
        "        left, right, top, bottom = x1 * im_width, x2 * im_width, y1 * im_height, y2 * im_height\n",
        "        #img = Image.fromarray(image_np)\n",
        "        #cropped_image = img.crop((int(left),int(top),int(right),int(bottom)))\n",
        "        #bb_color = color_detection(cropped_image)\n",
        "        cv2.rectangle(image_np, (int(left),int(top)), (int(right),int(bottom)),(36,255,12), 2)\n",
        "        cv2.putText(image_np, str(labels[int(classes[i])-1])+\":\"+str(int(scores[i]*100)), (int(left), int(top-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "    im_vid = cv2.resize(image_np,(1280,720))\n",
        "    out.write(im_vid)\n",
        "  out.release()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2M5xeQ-W1DR"
      },
      "source": [
        "video_inference1('input.mp4')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHICTCp3szFf"
      },
      "source": [
        "def video_inference(video_file):\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  out_path = os.path.join('/content/drive/MyDrive/wobot_final/video/', 'output.webM')\n",
        "  size = (1280,720)\n",
        "  fps = 10\n",
        "  fourcc = cv2.VideoWriter_fourcc('V','P','8','0')\n",
        "  out = cv2.VideoWriter(out_path,fourcc, fps, size)\n",
        "  labels = ['Helmet', 'Head', 'Person']\n",
        "\n",
        "  while cap.isOpened():\n",
        "    ret, image_np = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    output_dict = run_inference_for_single_image(model, image_np)\n",
        "    dets = output_dict['detection_boxes']\n",
        "    classes = output_dict['detection_classes']\n",
        "    scores = output_dict['detection_scores']\n",
        "    im_height = image_np.shape[0]\n",
        "    im_width = image_np.shape[1]\n",
        "\n",
        "\n",
        "    for i in range(len(dets[0])):\n",
        "      if (scores[i] > 0.2) and (len(classes) != 0):\n",
        "        y1, x1, y2, x2 = dets[i,:]\n",
        "        left, right, top, bottom = x1 * im_width, x2 * im_width, y1 * im_height, y2 * im_height\n",
        "        img = Image.fromarray(image_np)\n",
        "        cropped_image = img.crop((int(left),int(top),int(right),int(bottom)))\n",
        "        bb_color = color_detection(cropped_image)\n",
        "        cv2.rectangle(image_np, (int(left),int(top)), (int(right),int(bottom)), bb_color, 2)\n",
        "        cv2.putText(image_np, str(labels[int(classes[i])-1])+\":\"+str(int(scores[i]*100)), (int(left), int(top-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "    im_vid = cv2.resize(image_np,(1280,720))\n",
        "    out.write(im_vid)\n",
        "  out.release()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSBTUwTF-ZT-"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from sklearn.cluster import KMeans\n",
        "import time\n",
        "import scipy.misc"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7geUbIJCjlc"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b705cvS3Mf5n",
        "outputId": "0f8077eb-24be-4b85-93a1-44663ff8a225"
      },
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.5.3.56)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOKuUhztTglD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f527bea6-e2ef-43d7-a48b-27fdfa9b7c53"
      },
      "source": [
        "!pip install ffmpeg-python"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZstY-CVcNCt3"
      },
      "source": [
        "video_inference('input.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjiDubvyXKgP"
      },
      "source": [
        "import ffmpeg"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btHDz4IRXK9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4731ddca-631b-47fc-f147-f092a5ccb903"
      },
      "source": [
        "!ffmpeg -i  'output1.webM' -vf scale=1280:720 'output_1.mp4' "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, matroska,webm, from 'output1.webM':\n",
            "  Metadata:\n",
            "    ENCODER         : Lavf58.35.100\n",
            "  Duration: 00:27:46.30, start: 0.000000, bitrate: 578 kb/s\n",
            "    Stream #0:0: Video: vp8, yuv420p(progressive), 1280x720, SAR 1:1 DAR 16:9, 10 fps, 10 tbr, 1k tbn, 1k tbc (default)\n",
            "    Metadata:\n",
            "      DURATION        : 00:27:46.300000000\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (vp8 (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output_1.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 10 fps, 10240 tbn, 10 tbc (default)\n",
            "    Metadata:\n",
            "      DURATION        : 00:27:46.300000000\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=16663 fps= 15 q=-1.0 Lsize=  206496kB time=00:27:46.00 bitrate=1015.4kbits/s speed=1.55x    \n",
            "video:206318kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.086576%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mframe I:95    Avg QP:15.34  size: 74232\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mframe P:7005  Avg QP:19.70  size: 21637\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mframe B:9563  Avg QP:23.49  size:  5506\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mconsecutive B-frames: 18.3% 11.7% 11.1% 58.8%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mmb I  I16..4: 20.0% 45.6% 34.4%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mmb P  I16..4:  6.9% 15.6%  4.0%  P16..4: 27.5%  9.7%  4.2%  0.0%  0.0%    skip:32.1%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mmb B  I16..4:  1.1%  1.3%  0.4%  B16..8: 29.4%  3.8%  0.7%  direct: 3.0%  skip:60.2%  L0:56.5% L1:38.4% BI: 5.1%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0m8x8 transform intra:56.8% inter:66.6%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mcoded y,uvDC,uvAC intra: 43.7% 50.7% 30.9% inter: 12.8% 13.9% 3.5%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mi16 v,h,dc,p: 48% 27% 13% 12%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 36% 23% 21%  3%  3%  4%  4%  4%  4%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 37% 25% 13%  3%  5%  5%  5%  4%  3%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mi8c dc,h,v,p: 47% 22% 26%  5%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mWeighted P-Frames: Y:6.9% UV:4.3%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mref P L0: 68.5% 14.1% 12.9%  4.5%  0.1%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mref B L0: 91.0%  7.8%  1.2%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mref B L1: 97.2%  2.8%\n",
            "\u001b[1;36m[libx264 @ 0x5615724cd900] \u001b[0mkb/s:1014.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll9iuh98Tr1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eee8282-6153-43f4-aeb5-90bc45ecd94a"
      },
      "source": [
        "!ffmpeg -i  'output.webM' -vf scale=1280:720 'output_2.mp4' "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, matroska,webm, from 'output.webM':\n",
            "  Metadata:\n",
            "    ENCODER         : Lavf58.61.100\n",
            "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: vp8, yuv420p(progressive), 1280x720, SAR 1:1 DAR 16:9, 10 fps, 10 tbr, 1k tbn, 1k tbc (default)\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (vp8 (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output_2.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 10 fps, 10240 tbn, 10 tbc (default)\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "\u001b[0;35m[matroska,webm @ 0x55d854f1a000] \u001b[0m\u001b[1;31mRead error\n",
            "frame= 2148 fps= 14 q=-1.0 Lsize=   26053kB time=00:03:34.50 bitrate= 995.0kbits/s speed=1.41x    \n",
            "video:26029kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.093381%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mframe I:28    Avg QP:15.43  size: 52088\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mframe P:807   Avg QP:19.57  size: 22477\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mframe B:1313  Avg QP:23.49  size:  5373\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mconsecutive B-frames: 14.2%  9.4% 10.5% 65.9%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mmb I  I16..4: 22.8% 49.8% 27.5%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mmb P  I16..4:  5.3% 13.7%  3.5%  P16..4: 34.5% 12.8%  5.5%  0.0%  0.0%    skip:24.8%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mmb B  I16..4:  0.9%  1.3%  0.3%  B16..8: 39.0%  3.5%  0.6%  direct: 3.7%  skip:50.7%  L0:56.9% L1:37.8% BI: 5.3%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0m8x8 transform intra:58.5% inter:70.7%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mcoded y,uvDC,uvAC intra: 51.9% 52.3% 29.8% inter: 14.4% 14.2% 2.5%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mi16 v,h,dc,p: 38% 35% 12% 15%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 30% 25% 19%  3%  4%  5%  4%  5%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 35% 29% 13%  3%  5%  5%  5%  3%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mi8c dc,h,v,p: 48% 24% 23%  5%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mWeighted P-Frames: Y:8.6% UV:5.5%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mref P L0: 67.9% 14.7% 13.3%  4.1%  0.1%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mref B L0: 92.6%  6.5%  0.9%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mref B L1: 97.5%  2.5%\n",
            "\u001b[1;36m[libx264 @ 0x55d854f3b900] \u001b[0mkb/s:992.65\n"
          ]
        }
      ]
    }
  ]
}